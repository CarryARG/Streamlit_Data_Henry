{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from minio import Minio\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Configuración de conexión a MySQL utilizando SQLAlchemy\n",
    "user = 'user_arcope'\n",
    "password = 'passgrupo3'\n",
    "host = 'redarcope.ddns.net'\n",
    "port = '3306'\n",
    "database = 'arcope_uber'\n",
    "\n",
    "# Conexión a MySQL utilizando SQLAlchemy\n",
    "db_engine = create_engine(f'mysql+pymysql://{user}:{password}@{host}:{port}/{database}?connect_timeout=60')\n",
    "\n",
    "# Configuración de MinIO\n",
    "MINIO_ENDPOINT = 'redarcope.ddns.net:9000'\n",
    "MINIO_ACCESS_KEY = 'andres'\n",
    "MINIO_SECRET_KEY = 'andresarcope01'\n",
    "MINIO_BUCKET_NAME = 'transport-bucket'\n",
    "\n",
    "# Configuración de los datasets\n",
    "DATASETS_CONFIG = [\n",
    "    {'file': 'trips.parquet', 'table': 'trips', 'unique_column': 'unique_trips'},\n",
    "    {'file': 'vehicles.parquet', 'table': 'vehicles', 'unique_column': 'unique_vehicles'},\n",
    "    {'file': 'Geographic_Data.parquet', 'table': 'geographic_data', 'unique_column': 'unique_geographic'},\n",
    "    {'file': 'Meteorological_Data.parquet', 'table': 'meteorological_data', 'unique_column': 'unique_meteorological'},\n",
    "    {'file': 'fuel_stations.parquet', 'table': 'fuel_stations', 'unique_column': 'unique_stations'},\n",
    "    {'file': 'Fuel_Economy_Data.parquet', 'table': 'fuel_economy', 'unique_column': 'unique_economy'},\n",
    "    {'file': 'air_quality_measurement.parquet', 'table': 'air_quality_measurement', 'unique_column': 'unique_air'},\n",
    "    {'file': 'car_resale_prices.parquet', 'table': 'car_resale_prices', 'unique_column': 'unique_resale_price'}\n",
    "]\n",
    "\n",
    "# Función para extraer datos desde MinIO\n",
    "def extract_from_minio(file, **kwargs):\n",
    "    client = Minio(MINIO_ENDPOINT, access_key=MINIO_ACCESS_KEY, secret_key=MINIO_SECRET_KEY, secure=False)\n",
    "    response = client.get_object(MINIO_BUCKET_NAME, file)\n",
    "    \n",
    "    # Crear un archivo temporal en una ruta segura\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.parquet')\n",
    "    with open(temp_file.name, 'wb') as file_data:\n",
    "        for d in response.stream(32*1024):\n",
    "            file_data.write(d)\n",
    "    \n",
    "    # Guardar el nombre del archivo temporal en XCom para uso posterior\n",
    "    kwargs['ti'].xcom_push(key='temp_file_path', value=temp_file.name)\n",
    "    print(f\"Datos extraídos de {file}\")\n",
    "\n",
    "# Función para verificar si la columna existe en la tabla de MySQL\n",
    "def check_column_exists(table_name, column_name):\n",
    "    with db_engine.connect() as connection:\n",
    "        result = connection.execute(f\"DESCRIBE {table_name}\")\n",
    "        columns = [row['Field'] for row in result.fetchall()]\n",
    "        return column_name in columns\n",
    "\n",
    "# Función para filtrar datos nuevos basados en una columna única\n",
    "def filter_new_data(table_name, unique_column, **kwargs):\n",
    "    temp_file = kwargs['ti'].xcom_pull(key='temp_file_path', task_ids='extract_data')\n",
    "    df = pd.read_parquet(temp_file)\n",
    "    \n",
    "    # Verificar si la columna existe antes de intentar filtrar\n",
    "    if not check_column_exists(table_name, unique_column):\n",
    "        raise ValueError(f\"La columna '{unique_column}' no existe en la tabla '{table_name}'. Verifica el nombre de la columna.\")\n",
    "    \n",
    "    # Leer los datos existentes desde la tabla usando SQLAlchemy y un cursor\n",
    "    with db_engine.connect() as connection:\n",
    "        result = connection.execute(f\"SELECT {unique_column} FROM {table_name}\")\n",
    "        existing_data = pd.DataFrame(result.fetchall(), columns=[unique_column])\n",
    "    \n",
    "    # Filtrar los datos que no están presentes en la base de datos\n",
    "    new_data = df[~df[unique_column].isin(existing_data[unique_column])]\n",
    "    kwargs['ti'].xcom_push(key='new_data', value=new_data.to_json())  # Guardar los nuevos datos en XCom\n",
    "    print(f\"Datos nuevos filtrados para la tabla {table_name}\")\n",
    "\n",
    "# Función para simular la carga de datos a MySQL\n",
    "def load_data(table_name, **kwargs):\n",
    "    new_data_json = kwargs['ti'].xcom_pull(key='new_data', task_ids='filter_new_data')\n",
    "    new_data = pd.read_json(new_data_json)\n",
    "    \n",
    "    if not new_data.empty:\n",
    "        print(f\"Nuevos datos a cargar en la tabla {table_name}:\")\n",
    "        print(new_data.head())  # Muestra las primeras filas para verificar\n",
    "    else:\n",
    "        print(f\"No hay nuevos datos para cargar en la tabla {table_name}\")\n",
    "\n",
    "# Definición del DAG de Airflow\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id='etl_process_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='Pipeline ETL para extraer, transformar y cargar datos desde MinIO a MySQL',\n",
    "    schedule_interval=timedelta(days=1),\n",
    "    start_date=datetime(2024, 9, 26),\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    \n",
    "    for index, config in enumerate(DATASETS_CONFIG):\n",
    "        # Tareas del DAG para cada archivo de configuración\n",
    "\n",
    "        extract_data = PythonOperator(\n",
    "            task_id=f'extract_data_{index}',\n",
    "            python_callable=extract_from_minio,\n",
    "            op_kwargs={'file': config['file']},\n",
    "            provide_context=True,\n",
    "        )\n",
    "\n",
    "        filter_new_data = PythonOperator(\n",
    "            task_id=f'filter_new_data_{index}',\n",
    "            python_callable=filter_new_data,\n",
    "            op_kwargs={'table_name': config['table'], 'unique_column': config['unique_column']},\n",
    "            provide_context=True,\n",
    "        )\n",
    "\n",
    "        load_data = PythonOperator(\n",
    "            task_id=f'load_data_{index}',\n",
    "            python_callable=load_data,\n",
    "            op_kwargs={'table_name': config['table']},\n",
    "            provide_context=True,\n",
    "        )\n",
    "\n",
    "        # Definir el orden de ejecución de las tareas\n",
    "        extract_data >> filter_new_data >> load_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from minio import Minio\n",
    "from sqlalchemy import create_engine\n",
    "import tempfile\n",
    "\n",
    "# Configuración de MinIO y MySQL\n",
    "MINIO_ENDPOINT = 'redarcope.ddns.net:9000'\n",
    "MINIO_ACCESS_KEY = 'andres'\n",
    "MINIO_SECRET_KEY = 'andresarcope01'\n",
    "MINIO_BUCKET_NAME = 'transport-bucket'\n",
    "\n",
    "# Configuración de conexión a MySQL\n",
    "user = 'user_arcope'\n",
    "password = 'passgrupo3'\n",
    "host = 'redarcope.ddns.net'\n",
    "port = '3306'\n",
    "database = 'arcope_uber'\n",
    "\n",
    "# Conexión a MySQL\n",
    "db_engine = create_engine(f'mysql+pymysql://{user}:{password}@{host}:{port}/{database}?connect_timeout=60')\n",
    "\n",
    "# Configuración de los datasets\n",
    "DATASETS_CONFIG = [\n",
    "    {'file': 'trips.parquet', 'table': 'trips', 'unique_column': 'unique_trips'},\n",
    "    {'file': 'vehicles.parquet', 'table': 'vehicles', 'unique_column': 'unique_vehicles'},\n",
    "    {'file': 'Geographic_Data.parquet', 'table': 'geographic_data', 'unique_column': 'unique_geographic'},\n",
    "    {'file': 'Meteorological_Data.parquet', 'table': 'meteorological_data', 'unique_column': 'unique_meteorological'},\n",
    "    {'file': 'fuel_stations.parquet', 'table': 'fuel_stations', 'unique_column': 'unique_stations'},\n",
    "    {'file': 'Fuel_Economy_Data.parquet', 'table': 'fuel_economy', 'unique_column': 'unique_economy'},\n",
    "    {'file': 'air_quality_measurement.parquet', 'table': 'air_quality_measurement', 'unique_column': 'unique_air'},\n",
    "    {'file': 'car_resale_prices.parquet', 'table': 'car_resale_prices', 'unique_column': 'unique_resale_price'}\n",
    "]\n",
    "# Función para extraer datos desde MinIO\n",
    "def extract_from_minio(file):\n",
    "    client = Minio(MINIO_ENDPOINT, access_key=MINIO_ACCESS_KEY, secret_key=MINIO_SECRET_KEY, secure=False)\n",
    "    response = client.get_object(MINIO_BUCKET_NAME, file)\n",
    "    \n",
    "    # Crear un archivo temporal en una ruta segura\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.parquet')\n",
    "    with open(temp_file.name, 'wb') as file_data:\n",
    "        for d in response.stream(32*1024):\n",
    "            file_data.write(d)\n",
    "    \n",
    "    return temp_file.name\n",
    "\n",
    "# Función para verificar columnas en la tabla\n",
    "def check_column_exists(table_name, column_name):\n",
    "    with db_engine.connect() as connection:\n",
    "        # Obtener la lista de columnas de la tabla\n",
    "        result = connection.execute(f\"DESCRIBE {table_name}\")\n",
    "        columns = [row['Field'] for row in result.fetchall()]\n",
    "        # Verificar si la columna existe en la lista\n",
    "        return column_name in columns\n",
    "\n",
    "# Función para filtrar datos nuevos basados en una columna única\n",
    "def filter_new_data(df, table_name, unique_column):\n",
    "    # Verificar si la columna existe en la tabla antes de filtrar\n",
    "    if not check_column_exists(table_name, unique_column):\n",
    "        raise ValueError(f\"La columna '{unique_column}' no existe en la tabla '{table_name}'. Verifica el nombre de la columna.\")\n",
    "\n",
    "    # Leer los datos existentes desde la tabla usando SQLAlchemy y un cursor\n",
    "    with db_engine.connect() as connection:\n",
    "        result = connection.execute(f\"SELECT {unique_column} FROM {table_name}\")\n",
    "        existing_data = pd.DataFrame(result.fetchall(), columns=[unique_column])\n",
    "    \n",
    "    # Filtrar los datos que no están presentes en la base de datos\n",
    "    new_data = df[~df[unique_column].isin(existing_data[unique_column])]\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "# Función para simular la carga de datos a MySQL\n",
    "def load_data(new_data, table_name):\n",
    "    # Simular la carga de datos a la base de datos\n",
    "    if not new_data.empty:\n",
    "        print(f\"Nuevos datos a cargar en la tabla {table_name}:\")\n",
    "        print(new_data.head())  # Muestra las primeras filas para verificar\n",
    "    else:\n",
    "        print(f\"No hay nuevos datos para cargar en la tabla {table_name}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_12944\\3977580049.py:4: RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  result = connection.execute(f\"DESCRIBE {table_name}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['full_name', 'registered_year', 'transmission_type', 'fuel_type', 'max_power', 'resale_price', 'unique_resale_price']\n"
     ]
    }
   ],
   "source": [
    "def check_column_exist(table_name):\n",
    "    with db_engine.connect() as connection:\n",
    "        # Obtener la lista de columnas de la tabla\n",
    "        result = connection.execute(f\"DESCRIBE {table_name}\")\n",
    "        columns = [row['Field'] for row in result.fetchall()]\n",
    "    return(columns)\n",
    "\n",
    "index = 7\n",
    "config = DATASETS_CONFIG[index]\n",
    "\n",
    "#chequear columnas\n",
    "\n",
    "check= check_column_exist(config['table'])\n",
    "\n",
    "print(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos extraídos de car_resale_prices.parquet\n",
      "Nuevos datos a cargar en la tabla car_resale_prices:\n",
      "                      full_name registered_year transmission_type fuel_type  \\\n",
      "0  2017 Maruti Baleno 1.2 Alpha            2017            Manual    Petrol   \n",
      "1            2018 Tata Hexa XTA            2018         Automatic    Diesel   \n",
      "2   2015 Maruti Swift Dzire VXI            2015            Manual    Petrol   \n",
      "3   2015 Maruti Swift Dzire VXI            2015            Manual    Petrol   \n",
      "4    2009 Hyundai i10 Magna 1.1            2009            Manual    Petrol   \n",
      "\n",
      "   max_power resale_price                   unique_resale_price  \n",
      "0    83.1bhp  ₹ 5.45 Lakh  b9bf9c91-6502-4926-9cf6-05c66100757c  \n",
      "1  153.86bhp    ₹ 10 Lakh  233edf53-bada-4a39-abb4-50b1e2d7275d  \n",
      "2   83.14bhp  ₹ 4.50 Lakh  fb4324e4-6bad-4792-b3d1-c1d7ff313c43  \n",
      "3   83.14bhp  ₹ 4.50 Lakh  c1a05365-5b31-4c18-98ed-579d08e1f26f  \n",
      "4   68.05bhp  ₹ 1.60 Lakh  c1bd98f9-e236-4d0a-87ce-3a88cfd7b96f  \n",
      "Archivo temporal C:\\Users\\david\\AppData\\Local\\Temp\\tmpcc1ablht.parquet eliminado.\n"
     ]
    }
   ],
   "source": [
    "# Ejecución de las funciones para un dataset específico\n",
    "# Cambia 'index' para probar con otro dataset de la lista DATASETS_CONFIG\n",
    "index = 7\n",
    "config = DATASETS_CONFIG[index]\n",
    "\n",
    "# Extraer los datos\n",
    "temp_file = extract_from_minio(config['file'])\n",
    "print(f\"Datos extraídos de {config['file']}\")\n",
    "\n",
    "# Leer los datos extraídos en un DataFrame\n",
    "df = pd.read_parquet(temp_file)\n",
    "\n",
    "# Filtrar los datos nuevos\n",
    "new_data = filter_new_data(df, config['table'], config['unique_column'])\n",
    "\n",
    "# Simular la carga a MySQL\n",
    "load_data(new_data, config['table'])\n",
    "\n",
    "# Eliminar el archivo temporal\n",
    "os.remove(temp_file)\n",
    "print(f\"Archivo temporal {temp_file} eliminado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
