{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo de datos geográficos limpios\n",
    "geo_data = pd.read_parquet('Data/datos_geograficos_limpios.parquet')\n",
    "\n",
    "# Asegurarse de que los datos contienen las columnas necesarias\n",
    "required_columns = ['latitude', 'longitude', 'elevation', 'timezone']\n",
    "if not all(col in geo_data.columns for col in required_columns):\n",
    "    raise ValueError(f\"El archivo debe contener las columnas: {', '.join(required_columns)}\")\n",
    "\n",
    "# Crear el geo_join_id como identificador único\n",
    "geo_data['geo_join_id'] = range(1, len(geo_data) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV con los datos de costos operacionales de vehículos\n",
    "costo_operacional_vehiculos = pd.read_csv('Data/costo_operacional_vehiculos_clean.csv')\n",
    "\n",
    "# Crear el dataset Vehicles a partir del archivo CSV\n",
    "vehicles_dataset = costo_operacional_vehiculos[['Manuf', 'Model', 'Fuel_Type', 'Fuel_Cost', 'Electric_Cost', 'Total_Cost', 'Noise_Level']].copy()\n",
    "\n",
    "# Renombrar las columnas para que coincidan con la estructura deseada\n",
    "vehicles_dataset.rename(columns={\n",
    "    'Manuf': 'manuf',\n",
    "    'Model': 'model',\n",
    "    'Fuel_Type': 'fuel_type',\n",
    "    'Fuel_Cost': 'fuel_cost',\n",
    "    'Electric_Cost': 'electric_cost',\n",
    "    'Total_Cost': 'total_cost',\n",
    "    'Noise_Level': 'noise_level'\n",
    "}, inplace=True)\n",
    "\n",
    "# El campo 'model' será la clave primaria (PK)\n",
    "# Asegúrate de que 'model' sea único\n",
    "vehicles_dataset = vehicles_dataset.drop_duplicates(subset='model')\n",
    "\n",
    "# Guardar el nuevo dataset\n",
    "vehicles_dataset.to_parquet('etl_minio/vehicles.parquet', index=False)\n",
    "\n",
    "print(\"El dataset 'Vehicles' ha sido creado y guardado en 'vehicles.parquet'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trips (Yellow, Green, FHV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos\n",
    "yellow_tripdata = pd.read_parquet('Data/yellow_tripdata.parquet')\n",
    "green_tripdata = pd.read_parquet('Data/green_tripdata.parquet')\n",
    "fhv_tripdata = pd.read_parquet('Data/fhv_tripdata.parquet')\n",
    "geographic_data = pd.read_parquet('Data/datos_geograficos_limpios.parquet')\n",
    "meteorological_data = pd.read_parquet('Data/datos_meteorologicos_limpios.parquet')\n",
    "\n",
    "# Agregar la columna 'tipo' a cada DataFrame\n",
    "yellow_tripdata['tipo'] = 'Yellow'\n",
    "green_tripdata['tipo'] = 'Green'\n",
    "fhv_tripdata['tipo'] = 'FHV'\n",
    "\n",
    "# Crear DataFrame combinado\n",
    "combined_tripdata = pd.concat([yellow_tripdata, green_tripdata, fhv_tripdata], ignore_index=True)\n",
    "\n",
    "# Crear un DataFrame para las ubicaciones geográficas\n",
    "geo_data = geographic_data[['latitude', 'longitude', 'geo_join_id']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Unir el DataFrame combinado con datos geográficos por PULocationID y DOLocationID\n",
    "combined_tripdata = combined_tripdata.merge(geo_data, left_on='PULocationID', right_on='geo_join_id', suffixes=('', '_pickup'))\n",
    "combined_tripdata = combined_tripdata.merge(geo_data, left_on='DOLocationID', right_on='geo_join_id', suffixes=('', '_dropoff'))\n",
    "\n",
    "# Seleccionar y renombrar las columnas para el nuevo dataset\n",
    "combined_tripdata_dataset = combined_tripdata[['VendorID', 'pickup_datetime', 'dropoff_datetime', 'passenger_count', 'trip_distance', 'total_amount', 'PULocationID', 'DOLocationID', 'payment_type', 'congestion_surcharge', 'tipo']].copy()\n",
    "\n",
    "# Limpiar las columnas para la clave foránea\n",
    "combined_tripdata_dataset.rename(columns={\n",
    "    'VendorID': 'vendor_id',\n",
    "    'pickup_datetime': 'pickup_datetime',\n",
    "    'dropoff_datetime': 'dropoff_datetime',\n",
    "    'passenger_count': 'passenger_count',\n",
    "    'trip_distance': 'trip_distance',\n",
    "    'total_amount': 'total_amount',\n",
    "    'PULocationID': 'PULocationID',\n",
    "    'DOLocationID': 'DOLocationID',\n",
    "    'payment_type': 'payment_type',\n",
    "    'congestion_surcharge': 'congestion_surcharge',\n",
    "    'tipo': 'tipo'\n",
    "}, inplace=True)\n",
    "\n",
    "# Unir con datos meteorológicos por la fecha y hora de recogida\n",
    "combined_tripdata_dataset = combined_tripdata_dataset.merge(meteorological_data[['Time', 'Temperature_2m (°C)', 'rain (mm)', 'is_day']], left_on='pickup_datetime', right_on='Time', how='left')\n",
    "\n",
    "# Seleccionar las columnas necesarias\n",
    "combined_tripdata_dataset = combined_tripdata_dataset[['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'passenger_count', 'trip_distance', 'total_amount', 'PULocationID', 'DOLocationID', 'payment_type', 'congestion_surcharge', 'tipo', 'Temperature_2m (°C)', 'rain (mm)', 'is_day']]\n",
    "\n",
    "# Asegurarse de que los datos estén completos y eliminar duplicados\n",
    "combined_tripdata_dataset = combined_tripdata_dataset.drop_duplicates()\n",
    "\n",
    "# Guardar el nuevo dataset\n",
    "combined_tripdata_dataset.to_parquet('etl_minio/trips.parquet', index=False)\n",
    "\n",
    "print(\"El dataset combinado de viajes ha sido creado y guardado en 'trips.parquet'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos geográficos desde el archivo parquet\n",
    "geographic_data = pd.read_parquet('Data/datos_geograficos_limpios.parquet')\n",
    "\n",
    "# Seleccionar y renombrar las columnas para el nuevo dataset\n",
    "geographic_data_dataset = geographic_data[['geo_join_id', 'latitude', 'longitude', 'elevation', 'timezone']].copy()\n",
    "\n",
    "# Asegurarse de que 'geo_join_id' sea único (clave primaria)\n",
    "geographic_data_dataset.drop_duplicates(subset=['geo_join_id'], inplace=True)\n",
    "\n",
    "# Guardar el nuevo dataset\n",
    "geographic_data_dataset.to_parquet('etl_minio/geographic_data.parquet', index=False)\n",
    "\n",
    "print(\"El dataset de datos geográficos ha sido creado y guardado en 'geographic_data.parquet'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Air Quality Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo parquet con datos de calidad del aire\n",
    "air_quality = pd.read_parquet('../air_quality_cleaned.parquet')\n",
    "\n",
    "# Seleccionar y renombrar las columnas para el nuevo dataset\n",
    "air_quality_dataset = air_quality[['Nombre del Indicador', 'Medida del indicador', 'Geo Place Name', 'Time Period', 'Data Value', 'Year']].copy()\n",
    "\n",
    "# Renombrar las columnas para que coincidan con la estructura proporcionada\n",
    "air_quality_dataset.rename(columns={\n",
    "    'Nombre del Indicador': 'name',\n",
    "    'Medida del indicador': 'measure',\n",
    "    'Geo Place Name': 'geo_place_name',\n",
    "    'Time Period': 'time_period',\n",
    "    'Data Value': 'data_value',\n",
    "    'Year': 'year'\n",
    "}, inplace=True)\n",
    "\n",
    "# Cargar el dataset de datos geográficos para obtener geo_join_id\n",
    "geographic_data = pd.read_parquet('../datos_geograficos_limpios.parquet')\n",
    "\n",
    "# Asegurarse de que geo_place_name esté presente en ambos datasets para crear geo_join_id\n",
    "# Hacer una fusión para obtener geo_join_id\n",
    "merged_data = pd.merge(air_quality_dataset, geographic_data[['geo_place_name', 'geo_join_id']], \n",
    "                       on='geo_place_name', how='left')\n",
    "\n",
    "# Seleccionar y renombrar las columnas para el dataset final\n",
    "final_air_quality_dataset = merged_data[['indicator_id', 'name', 'measure', 'geo_join_id', 'data_value', 'time_period', 'year']].copy()\n",
    "\n",
    "# Asignar un identificador único para cada indicador (esto puede ser generado o asignado en otro proceso)\n",
    "final_air_quality_dataset['indicator_id'] = final_air_quality_dataset.index + 1\n",
    "\n",
    "# Asegurarse de que 'geo_join_id' esté presente y no tenga valores nulos\n",
    "final_air_quality_dataset = final_air_quality_dataset.dropna(subset=['geo_join_id'])\n",
    "\n",
    "# Guardar el nuevo dataset\n",
    "final_air_quality_dataset.to_parquet('../etl_minio/air_quality_measurement.parquet', index=False)\n",
    "\n",
    "print(\"El dataset de calidad del aire ha sido creado y guardado en 'air_quality_measurement.parquet'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Meteorological Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos meteorológicos desde el archivo parquet\n",
    "meteorological_data = pd.read_parquet('../datos_meteorologicos_limpios.parquet')\n",
    "\n",
    "# Seleccionar y renombrar las columnas para el nuevo dataset\n",
    "meteorological_data_dataset = meteorological_data[['Time', 'Temperature_2m (°C)', 'rain (mm)', 'is_day']].copy()\n",
    "\n",
    "# Renombrar las columnas para que coincidan con la estructura proporcionada\n",
    "meteorological_data_dataset.rename(columns={\n",
    "    'Time': 'time',\n",
    "    'Temperature_2m (°C)': 'temperature_2m',\n",
    "    'rain (mm)': 'rain',\n",
    "    'is_day': 'is_day'\n",
    "}, inplace=True)\n",
    "\n",
    "# Asegurarse de que 'time' sea único (clave primaria)\n",
    "meteorological_data_dataset.drop_duplicates(subset=['time'], inplace=True)\n",
    "\n",
    "# Guardar el nuevo dataset\n",
    "meteorological_data_dataset.to_parquet('../etl_minio/meteorological_data.parquet', index=False)\n",
    "\n",
    "print(\"El dataset de datos meteorológicos ha sido creado y guardado en 'meteorological_data.parquet'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuel Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos de estaciones de combustible desde el archivo parquet\n",
    "fuel_stations = pd.read_parquet('../df_eafcs.parquet')\n",
    "\n",
    "# Seleccionar y renombrar las columnas para el nuevo dataset\n",
    "fuel_stations_dataset = fuel_stations[['Station Name', 'Street Address', 'City', 'State', 'Fuel Type Code']].copy()\n",
    "\n",
    "# Renombrar las columnas para que coincidan con la estructura proporcionada\n",
    "fuel_stations_dataset.rename(columns={\n",
    "    'Station Name': 'station_name',\n",
    "    'Street Address': 'street_address',\n",
    "    'City': 'city',\n",
    "    'State': 'state',\n",
    "    'Fuel Type Code': 'fuel_type_code'\n",
    "}, inplace=True)\n",
    "\n",
    "# Asegurarse de que 'fuel_type_code' sea único (clave primaria)\n",
    "fuel_stations_dataset.drop_duplicates(subset=['fuel_type_code'], inplace=True)\n",
    "\n",
    "# Guardar el nuevo dataset\n",
    "fuel_stations_dataset.to_parquet('../etl_minio/fuel_stations.parquet', index=False)\n",
    "\n",
    "print(\"El dataset de estaciones de combustible ha sido creado y guardado en 'fuel_stations.parquet'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Fuel Economy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos de economía de combustible desde el archivo parquet\n",
    "fuel_economy_data = pd.read_parquet('../df_vfed.parquet')\n",
    "\n",
    "# Seleccionar y renombrar las columnas para el nuevo dataset\n",
    "fuel_economy_data_dataset = fuel_economy_data[['Manufacturer', 'Model', 'Miles per gallon (mpg)', 'CO2 (g p/mile)', 'FuelCost']].copy()\n",
    "\n",
    "# Renombrar las columnas para que coincidan con la estructura proporcionada\n",
    "fuel_economy_data_dataset.rename(columns={\n",
    "    'Manufacturer': 'manufacturer',\n",
    "    'Model': 'model',\n",
    "    'Miles per gallon (mpg)': 'miles_per_gallon',\n",
    "    'CO2 (g p/mile)': 'co2_per_mile',\n",
    "    'FuelCost': 'fuel_cost'\n",
    "}, inplace=True)\n",
    "\n",
    "# Asegurarse de que 'model' sea único (clave primaria)\n",
    "fuel_economy_data_dataset.drop_duplicates(subset=['model'], inplace=True)\n",
    "\n",
    "# Guardar el nuevo dataset\n",
    "fuel_economy_data_dataset.to_csv('../etl_minio/fuel_economy_data.parquet', index=False)\n",
    "\n",
    "print(\"El dataset de economía de combustible ha sido creado y guardado en 'fuel_economy_data.parquet'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
