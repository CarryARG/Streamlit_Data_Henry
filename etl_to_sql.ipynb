{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV con los datos de costos operacionales de vehículos\n",
    "costo_operacional_vehiculos = pd.read_csv('Data/costo_operacional_vehiculos_clean.csv')\n",
    "\n",
    "# Crear el dataset Vehicles a partir del archivo CSV\n",
    "vehicles_dataset = costo_operacional_vehiculos[['Manuf', 'Model', 'Fuel_Type', 'Fuel_Cost', 'Electric_Cost', 'Total_Cost', 'Noise_Level']].copy()\n",
    "\n",
    "# Renombrar las columnas para que coincidan con la estructura deseada\n",
    "vehicles_dataset.rename(columns={\n",
    "    'Manuf': 'manuf',\n",
    "    'Model': 'model',\n",
    "    'Fuel_Type': 'fuel_type',\n",
    "    'Fuel_Cost': 'fuel_cost',\n",
    "    'Electric_Cost': 'electric_cost',\n",
    "    'Total_Cost': 'total_cost',\n",
    "    'Noise_Level': 'noise_level'\n",
    "}, inplace=True)\n",
    "\n",
    "# El campo 'model' será la clave primaria (PK)\n",
    "# Asegúrate de que 'model' sea único\n",
    "vehicles_dataset = vehicles_dataset.drop_duplicates(subset='model')\n",
    "\n",
    "# Guardar el nuevo dataset\n",
    "vehicles_dataset.to_parquet('etl_minio/vehicles.parquet', index=False)\n",
    "\n",
    "print(\"El dataset 'Vehicles' ha sido creado y guardado en 'vehicles.parquet'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trips (Yellow, Green, FHV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Cargar los datos\n",
    "yellow_tripdata = pd.read_parquet('Data/yellow_tripdata.parquet')\n",
    "green_tripdata = pd.read_parquet('Data/green_tripdata.parquet')\n",
    "fhv_tripdata = pd.read_parquet('Data/fhv_tripdata.parquet')\n",
    "\n",
    "# Agregar la columna 'tipo' a cada DataFrame\n",
    "yellow_tripdata['tipo'] = 'Yellow'\n",
    "green_tripdata['tipo'] = 'Green'\n",
    "fhv_tripdata['tipo'] = 'FHV'\n",
    "\n",
    "# Crear DataFrame combinado\n",
    "combined_tripdata = pd.concat([yellow_tripdata, green_tripdata, fhv_tripdata], ignore_index=True)\n",
    "\n",
    "# Crear un DataFrame que combine todas las ubicaciones únicas\n",
    "unique_locations = pd.concat([\n",
    "   combined_tripdata[['PULocationID']].rename(columns={'PULocationID': 'LocationID'}),\n",
    "    combined_tripdata[['DOLocationID']].rename(columns={'DOLocationID': 'LocationID'})\n",
    "]).drop_duplicates()\n",
    " \n",
    "# Crear un geo_join_id único para cada LocationID\n",
    "unique_locations['geo_join_id'] = range(1, len(unique_locations) + 1)\n",
    "\n",
    "# Crear un mapping de LocationID a geo_join_id\n",
    "location_to_geo_id = unique_locations.set_index('LocationID')['geo_join_id'].to_dict()\n",
    "\n",
    "# Asignar geo_join_id a los datos de viajes\n",
    "combined_tripdata['PULocation_geo_join_id'] = combined_tripdata['PULocationID'].map(location_to_geo_id)\n",
    "combined_tripdata['DOLocation_geo_join_id'] = combined_tripdata['DOLocationID'].map(location_to_geo_id)\n",
    "\n",
    "\n",
    "# Seleccionar y renombrar las columnas para el nuevo dataset\n",
    "tripdata_dataset = combined_tripdata[['VendorID', 'pickup_datetime', 'dropoff_datetime', 'passenger_count', 'trip_distance', 'total_amount', 'PULocation_geo_join_id', 'DOLocation_geo_join_id', 'payment_type', 'congestion_surcharge', 'tipo']].copy()\n",
    "\n",
    "# Limpiar las columnas para la clave foránea\n",
    "tripdata_dataset.rename(columns={\n",
    "    'VendorID': 'vendor_id',\n",
    "    'pickup_datetime': 'pickup_datetime',\n",
    "    'dropoff_datetime': 'dropoff_datetime',\n",
    "    'passenger_count': 'passenger_count',\n",
    "    'trip_distance': 'trip_distance',\n",
    "    'total_amount': 'total_amount',\n",
    "    'PULocation_geo_join_id': 'PULocationID',\n",
    "    'DOLocation_geo_join_id': 'DOLocationID',\n",
    "    'payment_type': 'payment_type',\n",
    "    'congestion_surcharge': 'congestion_surcharge'\n",
    "}, inplace=True)\n",
    "\n",
    "# Seleccionar las columnas necesarias\n",
    "tripdata_dataset =  tripdata_dataset[['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'passenger_count', 'trip_distance', 'total_amount', 'PULocationID', 'DOLocationID', 'payment_type', 'congestion_surcharge', 'tipo']]\n",
    "\n",
    "# Guardar el nuevo dataset\n",
    "\n",
    "# Especificar la compresión (snappy, gzip, brotli, etc.)\n",
    "compression_type = 'snappy'  # Cambiar por 'gzip', 'brotli', etc.\n",
    "\n",
    "# Convertir DataFrame a un Table de PyArrow\n",
    "table = pa.Table.from_pandas(tripdata_dataset)\n",
    "\n",
    "# Guardar como archivo Parquet con compresión\n",
    "pq.write_table(table, 'etl_minio/trips.parquet', compression=compression_type)\n",
    "\n",
    "unique_locations.to_parquet('Data\\datos_geograficos_limpios.parquet', index=False)\n",
    "\n",
    "print(\"El dataset 'Trips' ha sido creado y guardado en 'trips_dataset.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos geográficos desde el archivo parquet\n",
    "geographic_data = pd.read_parquet('Data/datos_geograficos_limpios.parquet')\n",
    "air_quality = pd.read_parquet('Data/air_quality_cleaned.parquet')\n",
    "\n",
    "\n",
    "# Asegurarse de que 'geo_join_id' sea único (clave primaria)\n",
    "geographic_data.drop_duplicates(subset=['geo_join_id'], inplace=True)\n",
    "\n",
    "# Guardar el nuevo dataset\n",
    "geographic_data.to_parquet('etl_minio/geographic_data.parquet', index=False)\n",
    "\n",
    "print(\"El dataset de datos geográficos ha sido creado y guardado en 'geographic_data.parquet'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Air Quality Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo parquet con datos de calidad del aire\n",
    "air_quality = pd.read_parquet('Data/air_quality_cleaned.parquet')\n",
    "\n",
    "# Seleccionar y renombrar las columnas para el nuevo dataset\n",
    "air_quality_dataset = air_quality[['Name', 'Measure', 'Geo Place Name', 'Time Period', 'Data Value', 'Year']].copy()\n",
    "\n",
    "# Renombrar las columnas para que coincidan con la estructura proporcionada\n",
    "air_quality_dataset.rename(columns={\n",
    "    'Name': 'name',\n",
    "    'Measure': 'measure',\n",
    "    'Geo Place Name': 'geo_place_name',\n",
    "    'Time Period': 'time_period',\n",
    "    'Data Value': 'data_value',\n",
    "    'Year': 'year'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "# Asignar un identificador único para cada indicador (esto puede ser generado o asignado en otro proceso)\n",
    "air_quality_dataset['indicator_id'] = air_quality_dataset.index + 1\n",
    "\n",
    "# Guardar el nuevo dataset\n",
    "air_quality_dataset.to_parquet('etl_minio/air_quality_measurement.parquet', index=False)\n",
    "\n",
    "print(\"El dataset de calidad del aire ha sido creado y guardado en 'air_quality_measurement.parquet'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Meteorological Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos meteorológicos desde el archivo parquet\n",
    "meteorological_data = pd.read_parquet('Data/datos_meteorologicos_limpios.parquet')\n",
    "\n",
    "# Seleccionar y renombrar las columnas para el nuevo dataset\n",
    "meteorological_data_dataset = meteorological_data[['time', 'temperature_2m (°C)', 'rain (mm)', 'is_day ()']].copy()\n",
    "\n",
    "# Renombrar las columnas para que coincidan con la estructura proporcionada\n",
    "meteorological_data_dataset.rename(columns={\n",
    "    'Time': 'time',\n",
    "    'Temperature_2m (°C)': 'temperature_2m',\n",
    "    'rain (mm)': 'rain',\n",
    "    'is_day': 'is_day'\n",
    "}, inplace=True)\n",
    "\n",
    "# Asegurarse de que 'time' sea único (clave primaria)\n",
    "meteorological_data_dataset.drop_duplicates(subset=['time'], inplace=True)\n",
    "\n",
    "# Guardar el nuevo dataset\n",
    "meteorological_data_dataset.to_parquet('etl_minio/meteorological_data.parquet', index=False)\n",
    "\n",
    "print(\"El dataset de datos meteorológicos ha sido creado y guardado en 'meteorological_data.parquet'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteorological_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuel Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos de estaciones de combustible desde el archivo parquet\n",
    "fuel_stations = pd.read_parquet('Data/df_eafcs.parquet')\n",
    "\n",
    "# Seleccionar y renombrar las columnas para el nuevo dataset\n",
    "fuel_stations_dataset = fuel_stations[['Station Name', 'Street Address', 'City', 'State', 'Fuel Type Code']].copy()\n",
    "\n",
    "# Renombrar las columnas para que coincidan con la estructura proporcionada\n",
    "fuel_stations_dataset.rename(columns={\n",
    "    'Station Name': 'station_name',\n",
    "    'Street Address': 'street_address',\n",
    "    'City': 'city',\n",
    "    'State': 'state',\n",
    "    'Fuel Type Code': 'fuel_type_code'\n",
    "}, inplace=True)\n",
    "\n",
    "# Asegurarse de que 'fuel_type_code' sea único (clave primaria)\n",
    "fuel_stations_dataset.drop_duplicates(subset=['fuel_type_code'], inplace=True)\n",
    "\n",
    "# Guardar el nuevo dataset\n",
    "fuel_stations_dataset.to_parquet('etl_minio/fuel_stations.parquet', index=False)\n",
    "\n",
    "print(\"El dataset de estaciones de combustible ha sido creado y guardado en 'fuel_stations.parquet'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Fuel Economy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset de economía de combustible ha sido creado y guardado en 'fuel_economy_data.parquet'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos de economía de combustible desde el archivo parquet\n",
    "fuel_economy_data = pd.read_parquet('Data/df_vfed.parquet')\n",
    "\n",
    "# Seleccionar y renombrar las columnas para el nuevo dataset\n",
    "fuel_economy_data_dataset = fuel_economy_data[['Manufacturer', 'Model', 'Miles per gallon (mpg)', 'CO2 (p/mile)', 'FuelCost']].copy()\n",
    "\n",
    "# Renombrar las columnas para que coincidan con la estructura proporcionada\n",
    "fuel_economy_data_dataset.rename(columns={\n",
    "    'Manufacturer': 'manufacturer',\n",
    "    'Model': 'model',\n",
    "    'Miles per gallon (mpg)': 'miles_per_gallon',\n",
    "    'CO2 (g p/mile)': 'co2_per_mile',\n",
    "    'FuelCost': 'fuel_cost'\n",
    "}, inplace=True)\n",
    "\n",
    "# Asegurarse de que 'model' sea único (clave primaria)\n",
    "fuel_economy_data_dataset.drop_duplicates(subset=['model'], inplace=True)\n",
    "\n",
    "# Guardar el nuevo dataset\n",
    "fuel_economy_data_dataset.to_parquet('etl_minio/fuel_economy_data.parquet', index=False)\n",
    "\n",
    "print(\"El dataset de economía de combustible ha sido creado y guardado en 'fuel_economy_data.parquet'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_economy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "trips = pd.read_parquet('etl_minio/trips.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import uuid\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Función para generar identificadores únicos\n",
    "def generate_unique_id():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "# Cargar datasets\n",
    "trips = pd.read_parquet('etl_minio/trips.parquet')\n",
    "vehicles = pd.read_parquet('etl_minio/vehicles.parquet')\n",
    "geographic_data = pd.read_parquet('etl_minio/geographic_data.parquet')\n",
    "meteorological_data = pd.read_parquet('etl_minio/meteorological_data.parquet')\n",
    "fuel_stations = pd.read_parquet('etl_minio/fuel_stations.parquet')\n",
    "fuel_economy = pd.read_parquet('etl_minio/fuel_economy_data.parquet')\n",
    "air_quality = pd.read_parquet('etl_minio/air_quality_measurement.parquet')\n",
    "\n",
    "# Agregar la columna 'unique_column' con identificadores únicos para cada fila\n",
    "trips['unique_trips'] = [generate_unique_id() for _ in range(len(trips))]\n",
    "vehicles['unique_vehicles'] = [generate_unique_id() for _ in range(len(vehicles))]\n",
    "geographic_data['unique_geographic'] = [generate_unique_id() for _ in range(len(geographic_data))]\n",
    "meteorological_data['unique_meteorological'] = [generate_unique_id() for _ in range(len(meteorological_data))]\n",
    "fuel_stations['unique_stations'] = [generate_unique_id() for _ in range(len(fuel_stations))]\n",
    "fuel_economy['unique_economy'] = [generate_unique_id() for _ in range(len(fuel_economy))]\n",
    "air_quality['unique_air'] = [generate_unique_id() for _ in range(len(air_quality))]\n",
    "\n",
    "# Guardar los datasets con la nueva columna\n",
    "# Especificar la compresión (snappy, gzip, brotli, etc.)\n",
    "compression_type = 'gzip'  # Cambiar por 'gzip', 'brotli', etc.\n",
    "\n",
    "# Convertir DataFrame a un Table de PyArrow\n",
    "table = pa.Table.from_pandas(trips)\n",
    "\n",
    "\n",
    "pq.write_table(table, 'etl_minio/trips.parquet', compression=compression_type)\n",
    "\n",
    "vehicles.to_parquet('etl_minio/vehicles.parquet', index=False)\n",
    "geographic_data.to_parquet('etl_minio/geographic_data.parquet', index=False)\n",
    "meteorological_data.to_parquet('etl_minio/meteorological_data.parquet', index=False)\n",
    "fuel_stations.to_parquet('etl_minio/fuel_stations.parquet', index=False)\n",
    "fuel_economy.to_parquet('etl_minio/fuel_economy_data.parquet', index=False)\n",
    "air_quality.to_parquet('etl_minio/air_quality_measurement.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo air_quality_measurement.parquet subido con éxito\n",
      "Archivo fuel_economy_data.parquet subido con éxito\n",
      "Archivo fuel_stations.parquet subido con éxito\n",
      "Archivo geographic_data.parquet subido con éxito\n",
      "Archivo meteorological_data.parquet subido con éxito\n",
      "Archivo trips.parquet subido con éxito\n",
      "Archivo vehicles.parquet subido con éxito\n"
     ]
    }
   ],
   "source": [
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import os\n",
    "\n",
    "# Configura tu cliente MinIO\n",
    "minio_client = Minio(\n",
    "    \"redarcope.ddns.net:9000\",  # Reemplaza con tu dirección de MinIO\n",
    "    access_key=\"andres\",  # Reemplaza con tu access key\n",
    "    secret_key=\"andresarcope01\",  # Reemplaza con tu secret key\n",
    "    secure=False  # Cambia a True si usas HTTPS\n",
    ")\n",
    "\n",
    "\n",
    "# Lista de archivos a subir\n",
    "files_to_upload = [\"etl_minio/air_quality_measurement.parquet\", \"etl_minio/fuel_economy_data.parquet\",\"etl_minio/fuel_stations.parquet\", \"etl_minio/geographic_data.parquet\", \"etl_minio/meteorological_data.parquet\", \"etl_minio/trips.parquet\", \"etl_minio/vehicles.parquet\"]\n",
    "bucket_name = \"transport-bucket\"\n",
    "\n",
    "# Verifica si el bucket existe y créalo si no\n",
    "try:\n",
    "    if not minio_client.bucket_exists(bucket_name):\n",
    "        minio_client.make_bucket(bucket_name)\n",
    "except S3Error as err:\n",
    "    print(\"Error al crear el bucket:\", err)\n",
    "\n",
    "\n",
    "\n",
    "for file_path in files_to_upload:\n",
    "    object_name = os.path.basename(file_path)  # Extrae el nombre del archivo\n",
    "    try:\n",
    "        minio_client.fput_object(bucket_name, object_name, file_path)\n",
    "        print(f\"Archivo {object_name} subido con éxito\")\n",
    "    except S3Error as err:\n",
    "        print(f\"Error al subir {object_name}: {err}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
